{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01:  scraping Twitter for incident response\n",
    "\n",
    "This notebook details the methods used to collect tweets related to traffic incidents in the Los Angeles metropolitan area.  The current configuration is set up to gather data before and after an incident on the interstate 10 freeway in El Monte, CA.  To run a custom query you should update the incident time and query term list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 as got\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweet fetching function\n",
    "- for each query, collect tweets between two timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getweets(query,t0,t1): \n",
    "    \n",
    "    # send get query useing getoldtweets3\n",
    "    tweetCriteria = got.manager.TweetCriteria().setQuerySearch(query)\\\n",
    "                                               .setSince(t0)\\\n",
    "                                               .setUntil(t1)\n",
    "    tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "    \n",
    "    # select desired features\n",
    "    features = {\n",
    "    'tweet': 'text',\n",
    "    'username': 'username',\n",
    "    'date': 'formatted_date',\n",
    "    }\n",
    "    \n",
    "    # instantiate data dictionary for new data\n",
    "    data = {}\n",
    "\n",
    "    # loop through \n",
    "    for title in features.keys():\n",
    "        data[title] = []\n",
    "\n",
    "    for i in tweets:\n",
    "        for col, attr in features.items():\n",
    "            var = eval('i.' + attr)\n",
    "            data[col].append(var)\n",
    "\n",
    "    return(pd.DataFrame.from_dict(data))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define incident time, desired time window, and list of query terms\n",
    "- update this cell for each new incident with the following:\n",
    "1. incident time as pandas datetime object\n",
    "2. starting time t0 to collect tweets from\n",
    "3. ending time t1 to collect tweets until\n",
    "4. list of query terms to search twitter for\n",
    "- to do combinations, assign a list of stems and leaves to combine in multiple permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 query items\n"
     ]
    }
   ],
   "source": [
    "# Set incident time\n",
    "incident_time = pd.to_datetime(\"2019-02-23 13:00:00-07:00\")\n",
    "\n",
    "# Set collection starting time (recommended: 6 hours vefore incident)\n",
    "t0 = \"2019-02-22T00:00:00-07:00\"\n",
    "\n",
    "# Set Tweet collection ending time (recommended: 6 hours after incident)\n",
    "t1 = \"2019-02-25T00:00:00-07:00\"\n",
    "\n",
    "# SET QUERY TERMS:  use a combination of main artery and intersection/exits\n",
    "\n",
    "# main artery keywords\n",
    "stems = [\n",
    "    'I-10',\n",
    "    'I-10 Eastbound',\n",
    "    'I-10 East',\n",
    "    'I-10 Westbound',\n",
    "    'I-10 West',\n",
    "    'i10',\n",
    "    'i10 Eastbound',\n",
    "    'i10 East',\n",
    "    'i10 Westbound',\n",
    "    'i10 West',\n",
    "    'Ten eastbound',\n",
    "    'Ten east',\n",
    "    'Ten westbound',\n",
    "    'Ten west',\n",
    "    'Santa Monica Fwy'\n",
    "]\n",
    "\n",
    "# intersection/exit keywords\n",
    "leaves = [\n",
    "    'El Monte',\n",
    "    'Durfee Ave',\n",
    "    'Durfee'\n",
    "    'I-605',\n",
    "    'i605',\n",
    "    'San Gabriel Valley Fwy',\n",
    "    'exit 30'\n",
    "]\n",
    "\n",
    "# combine stem and leaf terms as individual values\n",
    "query_terms = stems + leaves\n",
    "\n",
    "# concatenate combinations of stems and leaves\n",
    "for s in stems:\n",
    "    for l in leaves:\n",
    "        nu_term = s + ' ' + l\n",
    "        query_terms.append(nu_term)\n",
    "        \n",
    "print(f\"{len(query_terms)} query items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8574, 3)\n",
      "CPU times: user 47.5 s, sys: 2.22 s, total: 49.7 s\n",
      "Wall time: 7min 1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>username</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pique jogador</td>\n",
       "      <td>gabriel_agle</td>\n",
       "      <td>2019-02-24 23:59:11+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>girodrigs7</td>\n",
       "      <td>2019-02-24 23:58:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alguém me entende</td>\n",
       "      <td>lauraincg</td>\n",
       "      <td>2019-02-24 23:48:45+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>歌います。 きいてください…『つばさをください』</td>\n",
       "      <td>i10_oo</td>\n",
       "      <td>2019-02-24 23:47:40+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I just got really high and ate a whole bag of ...</td>\n",
       "      <td>_mvndiii</td>\n",
       "      <td>2019-02-24 23:47:23+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet      username  \\\n",
       "index                                                                    \n",
       "0                                          Pique jogador  gabriel_agle   \n",
       "1                                                           girodrigs7   \n",
       "2                                      alguém me entende     lauraincg   \n",
       "3                               歌います。 きいてください…『つばさをください』        i10_oo   \n",
       "4      I just got really high and ate a whole bag of ...      _mvndiii   \n",
       "\n",
       "                           date  \n",
       "index                            \n",
       "0     2019-02-24 23:59:11+00:00  \n",
       "1     2019-02-24 23:58:45+00:00  \n",
       "2     2019-02-24 23:48:45+00:00  \n",
       "3     2019-02-24 23:47:40+00:00  \n",
       "4     2019-02-24 23:47:23+00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initialize query with 3-gram\n",
    "df_list = []\n",
    "\n",
    "# for each query term, add a new data frame to the list\n",
    "for q in query_terms:\n",
    "    nu_df = getweets(q,t0,t1)\n",
    "    df_list.append(nu_df)\n",
    "   \n",
    "# concatenate list of dataframes, set index, and convert dates to datetime objects\n",
    "data = pd.concat(df_list)  \n",
    "data['index'] = np.array(range(0,data.shape[0]))\n",
    "data = data.set_index('index')\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# display data attributes\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect language and filter out non-English tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = []\n",
    "\n",
    "# loop through tweets and detect language, add to list\n",
    "for tweet in list(data['tweet']):\n",
    "    try: \n",
    "        langs.append(detect(tweet))\n",
    "    except:\n",
    "        langs.append('unk')\n",
    "\n",
    "# add new column with languages\n",
    "data['langs'] = langs\n",
    "\n",
    "# filter for just English language Tweets and set corpus\n",
    "corpus = data[data['langs'] == 'en']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set corpus target by incident time\n",
    "- Tweets from before the incident are labeled with a 0\n",
    "- Tweets from after the incident are labeled with a 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sam/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.703302\n",
       "1    0.296698\n",
       "Name: after_incident, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set labels\n",
    "corpus['after_incident'] = [0 if time < incident_time else 1 for time in corpus['date']]\n",
    "\n",
    "# print normalized value counts for each class\n",
    "corpus['after_incident'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop unused columns and export csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define file output path\n",
    "outfile = './data/sample_output.csv'\n",
    "\n",
    "# drop date and language columns and export to csv\n",
    "corpus.drop(columns=['date','langs']).to_csv(outfile,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
